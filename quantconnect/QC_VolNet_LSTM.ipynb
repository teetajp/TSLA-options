{"cells":[{"cell_type":"markdown","metadata":{},"source":["![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n","<hr>"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#region imports\n","from AlgorithmImports import *\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","from sklearn.model_selection import train_test_split\n","import joblib\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import TensorDataset, DataLoader\n","#endregion"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# https://www.quantconnect.com/docs/v2/research-environment/machine-learning/pytorch\n","# Your New Python File\n","\n","# get historical data\n","qb = QuantBook()\n","symbol = qb.add_equity(\"TSLA\", Resolution.DAILY).symbol\n","history = qb.history(symbol, datetime(2019, 1, 1), datetime(2024, 1, 1)).loc[symbol]\n","\n","# Prepare data\n","df = pd.DataFrame(data={'close': history['close']})\n","df['log_returns'] = np.log(df['close']) - np.log(df['close'].shift(1))\n","\n","# calculate log returns and realized volatility of the underlying\n","prediction_window_size = 21  # trading days in rolling window # TODO: adjust this window 7, 10, 21.,...\n","dpy = 252  # trading days per year\n","# ann_factor = dpy / window_size # annualization factor\n","df['realized_vol_10'] = np.sqrt(df['log_returns'].rolling(window=10).var() * dpy / 10)\n","df['realized_vol_21'] = np.sqrt(df['log_returns'].rolling(window=21).var() * dpy / 21)\n","df['realized_vol_60'] = np.sqrt(df['log_returns'].rolling(window=60).var() * dpy / 60)\n","df = df.dropna()\n","df"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["last_n_days = 1000\n","\n","plt.figure(figsize=(14, 7))\n","# Plotting Log Returns\n","plt.subplot(3, 1, 1)\n","plt.plot(df['close'][-last_n_days:], label='TSLA Stock Close Price')\n","plt.title('TSLA Stock Prices')\n","plt.xlabel('Date')\n","plt.ylabel('Closing Price')\n","plt.grid(True)\n","\n","# Plotting Log Returns\n","plt.subplot(3, 1, 2)\n","plt.plot(df['log_returns'][-last_n_days:], label='Log Returns')\n","plt.title('Daily Log Returns')\n","plt.xlabel('Date')\n","plt.ylabel('Log Return')\n","plt.grid(True)\n","\n","# Plotting Realized Volatility\n","plt.subplot(3, 1, 3)\n","plt.plot(df['realized_vol_10'][-last_n_days:], label='Realized 10-day Volatility', color='red')\n","plt.plot(df['realized_vol_21'][-last_n_days:], label='Realized 21-day Volatility', color='green')\n","plt.plot(df['realized_vol_60'][-last_n_days:], label='Realized 60-day Volatility', color='blue')\n","plt.title('Realized k-Day Volatility')\n","plt.xlabel('Date')\n","plt.ylabel('Volatility')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Feature selection\n","feature_to_predict = 'realized_vol_10'\n","# get column index of feature_to_predict\n","label_idx = df.columns.get_loc(feature_to_predict)\n","\n","# set lookback window size: how many days of past data does the LSTM look at?\n","lookback = 10\n","\n","# PyTorch settings\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","torch.set_default_dtype(torch.float64)\n","\n","# create set creation\n","def create_dataset(df, lookback, label_idx, device):\n","    features = []\n","    labels = []\n","    for i in range(len(df)-lookback):\n","        features.append(df[i:i+lookback]) #  indices `i` up to (but excluding) `i+lookback`\n","        labels.append(df[i+lookback][label_idx]) # index `i+lookback` only\n","\n","    return torch.tensor(features.array()).to(device), torch.tensor(labels.array()).to(device)\n","\n","# split dataset into train and test sets\n","split_idx = int(df.shape[0] * 0.8)\n","train_df, test_df = df[:split_idx], df[split_idx:]\n","\n","# since LSTM uses sigmoid or tanh activation, it is senstivie to scale of the data, should rescale to [0, 1]\n","scaler = MinMaxScaler(feature_range=(0,1))\n","train_df_scaled = scaler.fit_transform(train_df)\n","test_df_scaled = scaler.transform(test_df)\n","print(train_df_scaled.shape, test_df_scaled.shape)\n","\n","# split into input features X and outputlabels y\n","X_train, y_train = create_dataset(train_df_scaled, lookback, label_idx, device)\n","X_test, y_test = create_dataset(test_df_scaled, lookback, label_idx, device)\n","\n","# create TensorDatasets\n","train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["len(train_dataset), len(test_dataset)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["train_dataset = TensorDataset(X_train, y_train)\n","test_dataset = TensorDataset(X_test, y_test)\n","\n","batch_size = 8\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["class VolNet(nn.Module):\n","    def __init__(self, num_inputs):\n","        super(VolNet, self).__init__()\n","        self.num_inputs = num_inputs\n","        self.num_layers = 2\n","        self.hidden_size = 32\n","        self.num_outputs = 1\n","        \n","        self.lstm = nn.LSTM(input_size=num_inputs, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True) # up to 3 layers for LSTM\n","        self.dense = nn.Linear(in_features=self.hidden_size, out_features=self.num_outputs) # fully-connected layer\n","        \n","        # self.last_h = self.initHidden()\n","        # self.last_c = self.initHidden()\n","        \n","    def forward(self, x):\n","        # x should be of shape (batch, sequence, features)\n","        batch_size = x.shape[0]\n","        lstm_out, (hn, cn) = self.lstm(x, (self.initHidden(batch_size), self.initHidden(batch_size)))\n","        # self.last_h, self.last_c = hn, cn\n","        last_time_step_out = lstm_out[:, -1, :]  # Use the output from the last time step\n","        return self.dense(last_time_step_out)\n","    \n","    def initHidden(self, batch_size):\n","        if not batch_size:\n","            batch_size = self.hidden_size * self.num_layers\n","        return torch.zeros(self.num_layers, batch_size, self.hidden_size).requires_grad_()\n","        \n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = VolNet(num_inputs=X_train.shape[-1]).to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model.train()  # Set model to training mode\n","    total_loss = 0\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        # Forward pass\n","        outputs = model(inputs).squeeze()\n","        labels.squeeze_()\n","        loss = criterion(outputs, labels)\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","            \n","            \n","    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}')\n","\n","# save model as to `joblib`\n","# Accoridng to Quantconnect docs,\n","# \"Don't use the torch.save method to save models because the tensor data will be lost and corrupt the save.\"\n","model_key = \"LSTM_model\"\n","file_name = qb.object_store.get_file_path(model_key)\n","joblib.dump(model, file_name)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Testing Phase\n","# Assuming test_dataset is an instance of TensorDataset or similar\n","\n","test_yhat = []\n","test_y = []\n","\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()  # Set the model to evaluation mode\n","    total_loss = 0\n","    with torch.no_grad():  # Turn off gradients for validation, saves memory and computations\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs).squeeze()\n","            labels.squeeze_()\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            test_yhat.append(outputs.detach().cpu().numpy())\n","            test_y.append(labels.detach().cpu().numpy())\n","    \n","    avg_loss = total_loss / len(test_loader)\n","    print(f'Test Loss: {avg_loss:.4f}')\n","    return avg_loss\n","\n","\n","# Example of how you might call this function\n","test_loss = evaluate_model(model, test_loader, criterion)\n","\n","test_yhat = np.hstack(test_yhat)\n","test_y = np.hstack(test_y)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# put the predicted and actual realized volatility side by side to compare\n","test_compare = pd.DataFrame({'Predicted_RVol': test_yhat, 'Actual_RVol': test_y})\n","print(test_compare.to_string())\n","test_compare.plot(title=f'Model Performance: predicted vs actual normalized {feature_to_predict}', figsize=(15, 10))\n","plt.show()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# test model\n","predict = model(X_test)\n","y_predict = predict.detach().numpy()   # Convert tensor to numpy ndarray\n","y_test_np = y_test.detach().numpy()\n","\n","df_perf = pd.DataFrame({'Real': y_test_np.flatten(), 'Predicted': y_predict.flatten()})\n","df_perf.plot(title=f'Model Performance: predicted vs actual normalized {feature_to_predict}', figsize=(15, 10))\n","plt.show()\n","\n","r2 = 1 - np.sum(np.square(y_test_np.flatten() - y_predict.flatten())) / np.sum(np.square(y_test_np.flatten() - y_test_np.mean()))\n","print(f\"The explained variance by the model (r-square): {r2*100:.2f}%\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["y_test_np.flatten().shape, y_predict.flatten().shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# To load model use the following function with whatever `model_key` you saved the model as\n","def load_model(model_key=\"LSTM_model\"):\n","    qb.object_store.contains_key(model_key)\n","    file_name = qb.object_store.get_file_path(model_key)\n","    loaded_model = joblib.load(file_name)"]}],"metadata":{"kernelspec":{"display_name":"Foundation-Py-Default","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":2}
