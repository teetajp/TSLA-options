{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we will load the data in a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronje\\AppData\\Local\\Temp\\ipykernel_44924\\2320442091.py:4: DtypeWarning: Columns (8,9,10,11,12,15,17,18,20,21,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('tsla_2019_2022.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names: Index(['[QUOTE_UNIXTIME]', '[QUOTE_READTIME]', '[QUOTE_DATE]',\n",
      "       '[QUOTE_TIME_HOURS]', '[UNDERLYING_LAST]', '[EXPIRE_DATE]',\n",
      "       '[EXPIRE_UNIX]', '[DTE]', '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n",
      "       '[C_THETA]', '[C_RHO]', '[C_IV]', '[C_VOLUME]', '[C_LAST]', '[C_SIZE]',\n",
      "       '[C_BID]', '[C_ASK]', '[STRIKE]', '[P_BID]', '[P_ASK]', '[P_SIZE]',\n",
      "       '[P_LAST]', '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]',\n",
      "       '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[STRIKE_DISTANCE]',\n",
      "       '[STRIKE_DISTANCE_PCT]'],\n",
      "      dtype='object')\n",
      "   [QUOTE_UNIXTIME]   [QUOTE_READTIME] [QUOTE_DATE]  [QUOTE_TIME_HOURS]  \\\n",
      "0        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "1        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "2        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "3        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "4        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "\n",
      "   [UNDERLYING_LAST] [EXPIRE_DATE]  [EXPIRE_UNIX]  [DTE]  [C_DELTA]  \\\n",
      "0             233.98    2019-05-03     1556913600    2.0   0.984650   \n",
      "1             233.98    2019-05-03     1556913600    2.0   0.983710   \n",
      "2             233.98    2019-05-03     1556913600    2.0   0.984580   \n",
      "3             233.98    2019-05-03     1556913600    2.0   0.991870   \n",
      "4             233.98    2019-05-03     1556913600    2.0   0.993410   \n",
      "\n",
      "   [C_GAMMA]  ...   [P_LAST]   [P_DELTA]  [P_GAMMA]   [P_VEGA]   [P_THETA]  \\\n",
      "0   0.000550  ...   0.010000   -0.000710   0.000030   0.000460   -0.009750   \n",
      "1   0.000670  ...   0.020000   -0.001090   0.000090   0.000580   -0.010100   \n",
      "2   0.000690  ...   0.020000   -0.001220   0.000120   0.001130   -0.014350   \n",
      "3   0.000490  ...   0.010000   -0.001340   0.000090   0.001390   -0.014650   \n",
      "4   0.000390  ...   0.010000   -0.001760   0.000080   0.001050   -0.014670   \n",
      "\n",
      "      [P_RHO]     [P_IV]   [P_VOLUME] [STRIKE_DISTANCE]  [STRIKE_DISTANCE_PCT]  \n",
      "0    0.000000   2.225480   147.000000             104.0                  0.444  \n",
      "1   -0.000010   2.083490    12.000000              99.0                  0.423  \n",
      "2    0.000000   2.023590    15.000000              94.0                  0.402  \n",
      "3    0.000000   1.895040     0.000000              89.0                  0.380  \n",
      "4    0.000000   1.768120    91.000000              84.0                  0.359  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Training data shape: (1329629, 33)\n",
      "Testing data shape: (1329630, 33)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv('tsla_2019_2022.csv')\n",
    "\n",
    "# Remove leading and trailing whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Display the column names\n",
    "print(\"Column names:\", data.columns)\n",
    "\n",
    "# Display the first few rows of the DataFrame to ensure it loaded correctly\n",
    "print(data.head())\n",
    "\n",
    "# Split the data into training and testing sets (assuming the data is already sorted by date)\n",
    "split_index = len(data) // 2\n",
    "train_data = data.iloc[:split_index]\n",
    "test_data = data.iloc[split_index:]\n",
    "\n",
    "# Verify the shape of the training and testing sets\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to parse for ATM strikes. To do this, I will only be looking at options where the delta is between 45-55, or -45 to -55. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [QUOTE_UNIXTIME]   [QUOTE_READTIME] [QUOTE_DATE]  [QUOTE_TIME_HOURS]  \\\n",
      "27         1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "98         1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "99         1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "181        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "275        1556740800   2019-05-01 16:00   2019-05-01                16.0   \n",
      "\n",
      "     [UNDERLYING_LAST] [EXPIRE_DATE]  [EXPIRE_UNIX]  [DTE]  [C_DELTA]  \\\n",
      "27              233.98    2019-05-03     1556913600    2.0    0.46754   \n",
      "98              233.98    2019-05-10     1557518400    9.0    0.54695   \n",
      "99              233.98    2019-05-10     1557518400    9.0    0.49653   \n",
      "181             233.98    2019-05-17     1558123200   16.0    0.50681   \n",
      "275             233.98    2019-05-24     1558728000   23.0    0.54600   \n",
      "\n",
      "     [C_GAMMA]  ...    [P_LAST] [P_DELTA]  [P_GAMMA]   [P_VEGA]   [P_THETA]  \\\n",
      "27    0.040110  ...    4.150000  -0.53568   0.042940   0.080630   -0.747790   \n",
      "98    0.019620  ...    6.890000  -0.45314   0.020360   0.151120   -0.407640   \n",
      "99    0.020350  ...    7.900000  -0.50511   0.021240   0.152720   -0.394080   \n",
      "181   0.015420  ...   10.360000  -0.49447   0.015910   0.199240   -0.303260   \n",
      "275   0.012650  ...   10.750000  -0.45426   0.013110   0.236670   -0.255550   \n",
      "\n",
      "        [P_RHO]     [P_IV]   [P_VOLUME] [STRIKE_DISTANCE]  \\\n",
      "27    -0.009430   0.456950                            1.0   \n",
      "98    -0.029230   0.510130   903.000000               1.5   \n",
      "99    -0.032080   0.490250                            1.0   \n",
      "181   -0.055160   0.500070                            1.0   \n",
      "275   -0.073160   0.508710   101.000000               1.5   \n",
      "\n",
      "     [STRIKE_DISTANCE_PCT]  \n",
      "27                   0.004  \n",
      "98                   0.006  \n",
      "99                   0.004  \n",
      "181                  0.004  \n",
      "275                  0.006  \n",
      "\n",
      "[5 rows x 33 columns]\n",
      "Number of rows in atm_options: 119965\n"
     ]
    }
   ],
   "source": [
    "# Convert '[C_DELTA]' and '[P_DELTA]' columns to numeric data types\n",
    "data['[C_DELTA]'] = pd.to_numeric(data['[C_DELTA]'], errors='coerce')\n",
    "data['[P_DELTA]'] = pd.to_numeric(data['[P_DELTA]'], errors='coerce')\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = ['QUOTE_READTIME', 'DTE', '[C_DELTA]', '[C_IV]', 'STRIKE', '[P_DELTA]', '[P_IV]']\n",
    "\n",
    "# Filter data to include only ATM options (where delta is closest to 0.5)\n",
    "# Ensure column names match exactly what's in the DataFrame\n",
    "atm_options = data[(data['[C_DELTA]'] >= 0.45) & (data['[C_DELTA]'] <= 0.55) & (data['[P_DELTA]'] >= -0.55) & (data['[P_DELTA]'] <= -0.45)]\n",
    "\n",
    "# Display the first few rows of the filtered DataFrame\n",
    "print(atm_options.head())\n",
    "print(\"Number of rows in atm_options:\", len(atm_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now normalize the data and organize it for our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "import sys\n",
    "sys.executable\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of atm_options: (119965, 33)\n",
      "Selected features: ['DTE', 'C_DELTA', 'C_IV', 'P_DELTA', 'P_IV']\n",
      "Number of rows in atm_options after filtering: 119965\n",
      "Shape of X: (119955, 10, 5)\n",
      "Shape of y: (119955, 5)\n"
     ]
    }
   ],
   "source": [
    "# Remove square brackets from column names\n",
    "atm_options.columns = atm_options.columns.str.strip('[]')\n",
    "\n",
    "# Now you can access columns without square brackets\n",
    "selected_features = ['DTE', 'C_DELTA', 'C_IV', 'P_DELTA', 'P_IV']\n",
    "\n",
    "\n",
    "print(\"Shape of atm_options:\", atm_options.shape)\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"Number of rows in atm_options after filtering:\", len(atm_options))\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(atm_options[selected_features])\n",
    "\n",
    "# Organize the data into sequences with a variable length\n",
    "X = []\n",
    "y = []\n",
    "sequence_length = 10\n",
    "\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i+sequence_length])\n",
    "    y.append(scaled_data[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Print the shape of X and y to verify\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train a model to predict IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Collecting tensorflow-intel==2.16.1\n",
      "  Downloading tensorflow_intel-2.16.1-cp310-cp310-win_amd64.whl (376.9 MB)\n",
      "     -------------------------------------- 376.9/376.9 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "     ---------------------------------------- 26.4/26.4 MB 6.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.24.3)\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-win_amd64.whl (127 kB)\n",
      "     -------------------------------------- 127.8/127.8 KB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ronje\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-win_amd64.whl (37 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 KB ? eta 0:00:00\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.5.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras>=3.0.0\n",
      "  Downloading keras-3.2.0-py3-none-any.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (58.1.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.10.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 5.5/5.5 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "     ------------------------------------- 413.4/413.4 KB 12.6 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 KB 3.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\ronje\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "     -------------------------------------- 133.7/133.7 KB 8.2 MB/s eta 0:00:00\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.43.0-py3-none-any.whl (65 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.11.0-cp310-cp310-win_amd64.whl (243 kB)\n",
      "     -------------------------------------- 243.8/243.8 KB 5.0 MB/s eta 0:00:00\n",
      "Collecting rich\n",
      "  Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "     -------------------------------------- 240.7/240.7 KB 7.4 MB/s eta 0:00:00\n",
      "Collecting namex\n",
      "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.1.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "     ---------------------------------------- 105.4/105.4 KB ? eta 0:00:00\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.2-py3-none-any.whl (226 kB)\n",
      "     ------------------------------------- 226.8/226.8 KB 14.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ronje\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ronje\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.14.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "     ---------------------------------------- 87.5/87.5 KB 4.8 MB/s eta 0:00:00\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, absl-py, tensorboard, markdown-it-py, astunparse, rich, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.2.0 libclang-18.1.1 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 protobuf-4.25.3 rich-13.7.1 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.2 wheel-0.43.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script wheel.exe is installed in 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0330 - val_loss: 0.0195\n",
      "Epoch 2/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0168 - val_loss: 0.0142\n",
      "Epoch 3/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0132 - val_loss: 0.0133\n",
      "Epoch 4/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 6ms/step - loss: 0.0121 - val_loss: 0.0127\n",
      "Epoch 5/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0114 - val_loss: 0.0120\n",
      "Epoch 6/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0111 - val_loss: 0.0112\n",
      "Epoch 7/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 8/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0101 - val_loss: 0.0105\n",
      "Epoch 9/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 7ms/step - loss: 0.0096 - val_loss: 0.0117\n",
      "Epoch 10/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0094 - val_loss: 0.0099\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['DTE', 'C_DELTA', 'C_IV', 'P_DELTA', 'P_IV'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Selecting the same features for X_test as used for training\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Similarly, selecting the target variable (implied volatility) for y_test\u001b[39;00m\n\u001b[0;32m     22\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC_IV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP_IV\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Assuming you want to predict both C_IV and P_IV\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['DTE', 'C_DELTA', 'C_IV', 'P_DELTA', 'P_IV'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=50, input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(units=5)  # 5 output nodes for predicting each feature\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# Verify column names in the testing dataset\n",
    "print(\"Column names in test_data:\", test_data.columns)\n",
    "\n",
    "# Adjust selected_features if necessary to match column names in test_data\n",
    "selected_features = ['DTE', '[C_DELTA]', '[C_IV]', '[P_DELTA]', '[P_IV]']\n",
    "\n",
    "# Selecting the same features for X_test as used for training\n",
    "X_test = test_data[selected_features].values\n",
    "\n",
    "# Similarly, selecting the target variable (implied volatility) for y_test\n",
    "y_test = test_data[['[C_IV]', '[P_IV]']].values  # Assuming you want to predict both C_IV and P_IV\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Reshape the testing data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_test_flattened \u001b[38;5;241m=\u001b[39m \u001b[43mX_test\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m y_test_flattened \u001b[38;5;241m=\u001b[39m y_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing data\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot actual vs predicted implied volatility for each feature\n",
    "for i in range(predictions.shape[1]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test[:, i], label='Actual')\n",
    "    plt.plot(predictions[:, i], label='Predicted')\n",
    "    plt.title(f'Implied Volatility Prediction for Feature {selected_features[i]}')\n",
    "    plt.xlabel('Data Point')\n",
    "    plt.ylabel('Implied Volatility')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ronje\\AppData\\Local\\Temp\\ipykernel_44924\\2239400831.py:10: DtypeWarning: Columns (8,9,10,11,12,15,17,18,20,21,23,24,25,26,27,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('tsla_2019_2022.csv')\n",
      "c:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 5ms/step - loss: 0.0270 - val_loss: 0.0160\n",
      "Epoch 2/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0137 - val_loss: 0.0119\n",
      "Epoch 3/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0110 - val_loss: 0.0106\n",
      "Epoch 4/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 5ms/step - loss: 0.0100 - val_loss: 0.0104\n",
      "Epoch 5/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0095 - val_loss: 0.0099\n",
      "Epoch 6/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 7/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0086 - val_loss: 0.0091\n",
      "Epoch 8/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0081 - val_loss: 0.0089\n",
      "Epoch 9/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 0.0078 - val_loss: 0.0083\n",
      "Epoch 10/10\n",
      "\u001b[1m2999/2999\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0075 - val_loss: 0.0079\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test_data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[C_IV]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[P_IV]\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# Assuming you want to predict both C_IV and P_IV\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the testing set\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_loss)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Make predictions on the testing set\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ronje\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "data = pd.read_csv('tsla_2019_2022.csv')\n",
    "\n",
    "# Remove leading and trailing whitespace from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Convert '[C_DELTA]' and '[P_DELTA]' columns to numeric data types\n",
    "data['[C_DELTA]'] = pd.to_numeric(data['[C_DELTA]'], errors='coerce')\n",
    "data['[P_DELTA]'] = pd.to_numeric(data['[P_DELTA]'], errors='coerce')\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = ['[DTE]', '[C_DELTA]', '[C_IV]', '[STRIKE]', '[P_DELTA]', '[P_IV]']\n",
    "\n",
    "# Filter data to include only ATM options (where delta is closest to 0.5)\n",
    "atm_options = data[(data['[C_DELTA]'] >= 0.45) & (data['[C_DELTA]'] <= 0.55) & (data['[P_DELTA]'] >= -0.55) & (data['[P_DELTA]'] <= -0.45)]\n",
    "\n",
    "# Reset index of atm_options\n",
    "atm_options.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(atm_options[selected_columns])\n",
    "\n",
    "# Organize the data into sequences with a variable length\n",
    "X = []\n",
    "y = []\n",
    "sequence_length = 10\n",
    "\n",
    "for i in range(len(scaled_data) - sequence_length):\n",
    "    X.append(scaled_data[i:i+sequence_length])\n",
    "    y.append(scaled_data[i+sequence_length])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(units=50, input_shape=(X.shape[1], X.shape[2])),\n",
    "    Dense(units=6)  # 6 output nodes for predicting each feature\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "# Assuming you have loaded and preprocessed your test data similarly to the training data\n",
    "\n",
    "# Define X_test and y_test using the same process as for training data\n",
    "selected_features_test = ['[DTE]', '[C_DELTA]', '[C_IV]', '[STRIKE]', '[P_DELTA]', '[P_IV]']\n",
    "\n",
    "# Selecting the same features for X_test as used for training\n",
    "X_test = test_data[selected_features_test].values\n",
    "\n",
    "# Similarly, selecting the target variable (implied volatility) for y_test\n",
    "y_test = test_data[['[C_IV]', '[P_IV]']].values  # Assuming you want to predict both C_IV and P_IV\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Convert X_test and y_test to compatible data types\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot actual vs predicted implied volatility for each feature\n",
    "for i in range(predictions.shape[1]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_test[:, i], label='Actual')\n",
    "    plt.plot(predictions[:, i], label='Predicted')\n",
    "    plt.title(f'Implied Volatility Prediction for Feature {selected_features_test[i+2]}')\n",
    "    plt.xlabel('Data Point')\n",
    "    plt.ylabel('Implied Volatility')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['[QUOTE_UNIXTIME]', '[QUOTE_READTIME]', '[QUOTE_DATE]',\n",
      "       '[QUOTE_TIME_HOURS]', '[UNDERLYING_LAST]', '[EXPIRE_DATE]',\n",
      "       '[EXPIRE_UNIX]', '[DTE]', '[C_DELTA]', '[C_GAMMA]', '[C_VEGA]',\n",
      "       '[C_THETA]', '[C_RHO]', '[C_IV]', '[C_VOLUME]', '[C_LAST]', '[C_SIZE]',\n",
      "       '[C_BID]', '[C_ASK]', '[STRIKE]', '[P_BID]', '[P_ASK]', '[P_SIZE]',\n",
      "       '[P_LAST]', '[P_DELTA]', '[P_GAMMA]', '[P_VEGA]', '[P_THETA]',\n",
      "       '[P_RHO]', '[P_IV]', '[P_VOLUME]', '[STRIKE_DISTANCE]',\n",
      "       '[STRIKE_DISTANCE_PCT]'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#debugging cell\n",
    "print(atm_options.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
